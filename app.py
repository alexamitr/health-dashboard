# app.py â€” Multi-Model Health Dashboard (NOW + FUTURE)
# Î¤ÏÎ­Ï‡Ï‰ Î±Ï€ÏŒ Ï„ÎµÏÎ¼Î±Ï„Î¹ÎºÏŒ: streamlit run app.py

import streamlit as st
import pandas as pd
import numpy as np
import time
from io import StringIO
#  HTTP live:
try:
    import requests
except Exception:
    requests = None  


# Î¦Î­ÏÎ½Ï‰ Ï„Î¹Ï‚ ÏƒÏ…Î½Î±ÏÏ„Î®ÏƒÎµÎ¹Ï‚ Î±Ï€ÏŒ Ï„Î¿ pipeline.py (Ï€ÏÎ­Ï€ÎµÎ¹ Î½Î± ÎµÎ¯Î½Î±Î¹ ÏƒÏ„Î¿Î½ Î¯Î´Î¹Î¿ Ï†Î¬ÎºÎµÎ»Î¿)
from pipeline import (
    preprocess_for_inference,
    predict_all_models,
    compose_alerts_and_recs,
    make_alert_timeline,
    apply_alert_rule,
    summarize_for_dashboard,
)

# ==========================
# Î¡ÏÎ¸Î¼Î¹ÏƒÎ· ÏƒÎµÎ»Î¯Î´Î±Ï‚ / Î¤Î¯Ï„Î»Î¿Ï‚
# ==========================
st.set_page_config(page_title="Health Forecast Dashboard", layout="wide")
st.title("ğŸ›°ï¸ Health Forecast Dashboard â€” NOW & 5â€² FUTURE")

st.caption("Î¦ÏŒÏÏ„Ï‰ÏƒÎµ CSV Î¼Îµ raw Î¼ÎµÏ„ÏÎ®ÏƒÎµÎ¹Ï‚ â†’ preprocess (Î¯Î´Î¹Î¿ Î¼Îµ training) â†’ Ï„ÏÎ­Ï‡Ï‰ ÏŒÎ»Î± Ï„Î± Î¼Î¿Î½Ï„Î­Î»Î± â†’ KPIs, Î³ÏÎ±Ï†Î®Î¼Î±Ï„Î±, alerts & Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚.")

# ==========================================================
# DEFAULT MODEL SPECS (paths/thresholds â€” Ï€ÏÎ¿ÏƒÎ±ÏÎ¼ÏŒÎ¶ÎµÎ¹Ï‚ Î±Î½ Î¸ÎµÏ‚)
# ==========================================================
DEFAULT_MODEL_SPECS = [
    # FUTURE (5')
    {"name": "stress_future",      "model_path": "model_stress_event_future.pkl",
     "features_path": "features_model_stress_event_future.pkl", "threshold": 0.50, "kind": "FUTURE"},
    {"name": "fatigue_future",     "model_path": "model_fatigue_event_future.pkl",
     "features_path": "features_model_fatigue_event_future.pkl", "threshold": 0.50, "kind": "FUTURE"},
    {"name": "dehydration_future", "model_path": "model_dehydration_event_future.pkl",
     "features_path": "features_model_dehydration_event_future.pkl", "threshold": 0.50, "kind": "FUTURE"},
    {"name": "arrhythmia_future",  "model_path": "model_arrhythmia_event_future.pkl",
     "features_path": "features_model_arrhythmia_event_future.pkl", "threshold": 0.45, "kind": "FUTURE"},
    {"name": "hypothermia_future", "model_path": "model_hypothermia_event_future.pkl",
     "features_path": "features_model_hypothermia_event_future.pkl", "threshold": 0.45, "kind": "FUTURE"},
    {"name": "envrisk_future",     "model_path": "model_env_risk_event_future.pkl",
     "features_path": "features_model_env_risk_event_future.pkl", "threshold": 0.45, "kind": "FUTURE"},

    # NOW
    {"name": "stress_now", "model_path": "model_stress_event_now.pkl",
     "features_path": "features_stress_event_now.pkl", "threshold": 0.50, "kind": "NOW"},
    {"name": "envrisk_now", "model_path": "model_env_risk_event_now.pkl",
     "features_path": "features_env_risk_event_now.pkl", "threshold": 0.50, "kind": "NOW"},
    {"name": "arrhythmia_now", "model_path": "model_arrhythmia_event_now.pkl",
     "features_path": "features_arrhythmia_event_now.pkl", "threshold": 0.40, "kind": "NOW"},
    {"name": "hypothermia_now", "model_path": "model_hypothermia_event.pkl",
     "features_path": "features_hypothermia_event_now.pkl", "threshold": 0.40, "kind": "NOW"},
    {"name": "fatigue_now", "model_path": "model_fatigue_event_now.pkl",
     "features_path": "features_fatigue_event_now.pkl", "threshold": 0.50, "kind": "NOW"},
    {"name": "dehydration_now", "model_path": "model_dehydration_event_now.pkl",
     "features_path": "features_model_dehydration_event_now.pkl", "threshold": 0.50, "kind": "NOW"}

]

# ==========================
# SIDEBAR â€” Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚ / Input
# ==========================
st.sidebar.header("Î¡Ï…Î¸Î¼Î¯ÏƒÎµÎ¹Ï‚")

st.sidebar.markdown("---")
live_mode = st.sidebar.toggle("ğŸ”´ Live mode", value=False, help="Î‘Ï…Ï„ÏŒÎ¼Î±Ï„Î· Î±Î½Î±Î½Î­Ï‰ÏƒÎ· Î±Î½Î¬ Î Î´ÎµÏ…Ï„ÎµÏÏŒÎ»ÎµÏ€Ï„Î±")

source_kind = st.sidebar.selectbox(
    "Live source",
    options=["CSV (append)", "HTTP JSON (GET)"],
    index=0,
    help="Î”Î¹Î¬Î»ÎµÎ¾Îµ Ï€Î·Î³Î® live Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½"
)

refresh_sec = st.sidebar.slider("Refresh (sec)", 1, 30, 5, 1)
buffer_minutes = st.sidebar.slider("Buffer (minutes)", 5, 120, 30, 5, help="Î ÏŒÏƒÎ¿ Î¹ÏƒÏ„Î¿ÏÎ¹ÎºÏŒ ÎºÏÎ±Ï„Î¬Ï‰ Î³Î¹Î± rolling/lag")


uploaded = st.sidebar.file_uploader(
    "Î¦ÏŒÏÏ„Ï‰ÏƒÎµ CSV (raw Î¼ÎµÏ„ÏÎ®ÏƒÎµÎ¹Ï‚)",
    type=["csv"],
    help="Î§ÏÎµÎ¹Î¬Î¶Î¿Î¼Î±Î¹: timestamp Î® sec_of_day, ÎºÎ±Î¹ HR, EDA, TEMP, Temperature, Humidity"
)

all_names = [m["name"] for m in DEFAULT_MODEL_SPECS]
selected = st.sidebar.multiselect(
    "Î•Ï€Î­Î»ÎµÎ¾Îµ Ï€Î¿Î¹Î± Î¼Î¿Î½Ï„Î­Î»Î± Î½Î± Ï„ÏÎ­Î¾Ï‰",
    options=all_names,
    default=all_names
)
model_specs = [m for m in DEFAULT_MODEL_SPECS if m["name"] in selected]

thr_offset = st.sidebar.slider(
    "Global threshold offset",
    -0.20, 0.20, 0.00, 0.01,
    help="Î ÏÎ¿ÏƒÎ¸Î­Ï„Ï‰/Î±Ï†Î±Î¹ÏÏ Î±Ï€ÏŒ Ï„Î± per-model thresholds (Î³Î¹Î± Î³ÏÎ®Î³Î¿ÏÎ¿ fine-tuning)"
)

# ==========================
# 1) Î”ÎµÎ´Î¿Î¼Î­Î½Î± (raw)
# ==========================

def read_live_csv(path: str) -> pd.DataFrame:
    """Î”Î¹Î±Î²Î¬Î¶Ï‰ ÎŸÎ›ÎŸ Ï„Î¿ CSV Ï€Î¿Ï… ÎµÎ½Î·Î¼ÎµÏÏÎ½ÎµÏ„Î±Î¹ (append) ÎºÎ±Î¹ ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†Ï‰ DataFrame."""
    df = pd.read_csv(path)
    if "timestamp" in df.columns:
        df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce")
    return df

def read_live_http(url: str) -> pd.DataFrame:
    """
    ÎšÎ¬Î½Ï‰ GET ÏƒÎµ endpoint Ï€Î¿Ï… Î³Ï…ÏÎ½Î¬ JSON (Î»Î¯ÏƒÏ„Î± Î±Ï€ÏŒ records) ÎºÎ±Î¹ Ï†Ï„Î¹Î¬Ï‡Î½Ï‰ DataFrame.
    Î ÎµÏÎ¹Î¼Î­Î½Ï‰ Ï€ÎµÎ´Î¯Î±: timestamp/sec_of_day, HR, EDA, TEMP, Temperature, Humidity.
    """
    if requests is None:
        raise RuntimeError("Î›ÎµÎ¯Ï€ÎµÎ¹ Ï„Î¿ package 'requests' (pip install requests).")
    r = requests.get(url, timeout=5)
    r.raise_for_status()
    data = r.json()
    df = pd.DataFrame(data)
    if "timestamp" in df.columns:
        df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce")
    return df

def append_new_rows(buffer: pd.DataFrame | None, new_df: pd.DataFrame) -> pd.DataFrame:
    """
    Î•Î½ÏÎ½Ï‰ buffer Î¼Îµ Î½Î­Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î±. Î‘Î½ Î­Ï‡Ï‰ timestamp, ÎºÏÎ±Ï„Î¬Ï‰ Î¼ÏŒÎ½Î¿ ÏŒÏƒÎ± ÎµÎ¯Î½Î±Î¹ Î½ÎµÏŒÏ„ÎµÏÎ±.
    Î‘Î½ Î´ÎµÎ½ Î­Ï‡Ï‰, ÎºÎ¬Î½Ï‰ Î±Ï€Î»ÏŒ concat.
    """
    if buffer is None or buffer.empty:
        return new_df.copy()

    if "timestamp" in new_df.columns and "timestamp" in buffer.columns:
        last_ts = buffer["timestamp"].max()
        fresh = new_df[new_df["timestamp"] > last_ts]
        return pd.concat([buffer, fresh], ignore_index=True)
    else:
        # fallback: ÏƒÎºÎ­Ï„Î¿ concat
        return pd.concat([buffer, new_df.iloc[len(buffer):]], ignore_index=True)

def trim_buffer_to_minutes(df: pd.DataFrame, minutes: int) -> pd.DataFrame:
    """ÎšÏÎ±Ï„Î¬Ï‰ Î¼ÏŒÎ½Î¿ Ï„Î± Ï„ÎµÎ»ÎµÏ…Ï„Î±Î¯Î± Î§ Î»ÎµÏ€Ï„Î¬, Î±Î½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ timestamp. Î‘Î»Î»Î¹ÏÏ‚ ÎµÏ€Î¹ÏƒÏ„ÏÎ­Ï†Ï‰ ÏŒÏ€Ï‰Ï‚ ÎµÎ¯Î½Î±Î¹."""
    if "timestamp" not in df.columns or df.empty:
        return df
    cutoff = df["timestamp"].max() - pd.Timedelta(minutes=minutes)
    return df[df["timestamp"] >= cutoff].reset_index(drop=True)




st.subheader("1) Î”ÎµÎ´Î¿Î¼Î­Î½Î± (raw)")

df_raw = None

if live_mode:
    st.info("Live mode ÎµÎ½ÎµÏÎ³ÏŒ â€” Î¸Î± Î±Î½Î±Î½ÎµÏÎ½Ï‰ Î±Ï…Ï„ÏŒÎ¼Î±Ï„Î±.")
    if source_kind == "CSV (append)":
        live_path = st.sidebar.text_input("CSV path", "live_feed.csv", help="Î”Î¹Î±Î´ÏÎ¿Î¼Î® Î±ÏÏ‡ÎµÎ¯Î¿Ï… Ï€Î¿Ï… ÎµÎ½Î·Î¼ÎµÏÏÎ½ÎµÏ„Î±Î¹")
        if live_path:
            try:
                # 1) Ï†ÏŒÏÏ„Ï‰ÏƒÎµ Ï€Î»Î®ÏÎ· Ï„ÏÎ­Ï‡Î¿Ï…ÏƒÎ± ÎµÎ¹ÎºÏŒÎ½Î±
                new_df = read_live_csv(live_path)
                # 2) ÎµÎ½Î·Î¼Î­ÏÏ‰ÏƒÎµ buffer ÏƒÏ„Î¿ session_state
                buf = st.session_state.get("raw_buffer", pd.DataFrame())
                df_raw = append_new_rows(buf, new_df)
                # 3) ÎºÎ¬Î½Îµ trim ÏƒÏ„Î¿ Ï€Î±ÏÎ¬Î¸Ï…ÏÎ¿ Î§ Î»ÎµÏ€Ï„ÏÎ½ (Î³Î¹Î± rolling/lag)
                df_raw = trim_buffer_to_minutes(df_raw, buffer_minutes)
                st.session_state["raw_buffer"] = df_raw.copy()
            except Exception as e:
                st.error(f"Î£Ï†Î¬Î»Î¼Î± Î±Î½Î¬Î³Î½Ï‰ÏƒÎ·Ï‚ live CSV: {e}")
                st.stop()
        else:
            st.warning("Î”ÏÏƒÎµ Î­Î³ÎºÏ…ÏÎ¿ CSV path.")
            st.stop()

    else:  # HTTP JSON (GET)
        live_url = st.sidebar.text_input("HTTP URL", "http://127.0.0.1:8000/latest")
        if live_url:
            try:
                new_df = read_live_http(live_url)
                buf = st.session_state.get("raw_buffer", pd.DataFrame())
                df_raw = append_new_rows(buf, new_df)
                df_raw = trim_buffer_to_minutes(df_raw, buffer_minutes)
                st.session_state["raw_buffer"] = df_raw.copy()
            except Exception as e:
                st.error(f"Î£Ï†Î¬Î»Î¼Î± HTTP: {e}")
                st.stop()
        else:
            st.warning("Î”ÏÏƒÎµ Î­Î³ÎºÏ…ÏÎ¿ URL.")
            st.stop()

else:
    # classic: upload ÎµÎ½ÏŒÏ‚ CSV & ÏƒÏ„Î±Ï„Î¹ÎºÎ® Î±Î½Î¬Î»Ï…ÏƒÎ· (ÏŒÏ€Ï‰Ï‚ ÎµÎ¯Ï‡ÎµÏ‚)
    uploaded = st.sidebar.file_uploader(
        "Î¦ÏŒÏÏ„Ï‰ÏƒÎµ CSV (raw Î¼ÎµÏ„ÏÎ®ÏƒÎµÎ¹Ï‚)",
        type=["csv"],
        help="Î§ÏÎµÎ¹Î¬Î¶Î¿Î¼Î±Î¹: timestamp Î® sec_of_day, HR, EDA, TEMP, Temperature, Humidity"
    )
    if uploaded is None:
        st.info("Î¦ÏŒÏÏ„Ï‰ÏƒÎµ Î­Î½Î± CSV Î±Ï€ÏŒ Ï„Î¿ sidebar Î³Î¹Î± Î½Î± ÏƒÏ…Î½ÎµÏ‡Î¯ÏƒÏ‰.")
        st.stop()
    try:
        df_raw = pd.read_csv(uploaded)
        if "timestamp" in df_raw.columns:
            df_raw["timestamp"] = pd.to_datetime(df_raw["timestamp"], errors="coerce")
    except Exception as e:
        st.error(f"Î£Ï†Î¬Î»Î¼Î± Î±Î½Î¬Î³Î½Ï‰ÏƒÎ·Ï‚ CSV: {e}")
        st.stop()

# ÎšÎ¿Î¹Î½Î® Ï€ÏÎ¿ÎµÏ€Î¹ÏƒÎºÏŒÏ€Î·ÏƒÎ·
st.write("Î”ÎµÎ¯Î³Î¼Î± raw:")
st.dataframe(df_raw.tail(20))  # ÏƒÏ„Î¿ live Î´ÎµÎ¯Ï‡Î½Ï‰ Ï„Î± Ï„ÎµÎ»ÎµÏ…Ï„Î±Î¯Î±


# ==========================
# 2) Preprocess (Î¯Î´Î¹Î¿ Î¼Îµ training)
# ==========================
st.subheader("2) Preprocess (Î¯Î´Î¹Î¿ Î¼Îµ training)")

try:
    df_eng = preprocess_for_inference(df_raw)  # Î¯Î´Î¹Î± features/Î¿Î½ÏŒÎ¼Î±Ï„Î±/NA drop Î¼Îµ training
    st.success(f"OK â€” engineered ÏƒÏ‡Î®Î¼Î±: {df_eng.shape[0]:,} Î³ÏÎ±Î¼Î¼Î­Ï‚ Ã— {df_eng.shape[1]:,} ÏƒÏ„Î®Î»ÎµÏ‚")
    st.dataframe(df_eng.head(20))
except Exception as e:
    st.error(f"Î£Ï†Î¬Î»Î¼Î± preprocess: {e}")
    st.stop()

# ==========================
# 3) Î ÏÏŒÎ²Î»ÎµÏˆÎ· (Ï€Î¿Î»Î»Î±Ï€Î»Î¬ Î¼Î¿Î½Ï„Î­Î»Î±)
# ==========================
st.subheader("3) Î ÏÏŒÎ²Î»ÎµÏˆÎ· (Ï€Î¿Î»Î»Î±Ï€Î»Î¬ Î¼Î¿Î½Ï„Î­Î»Î±)")

# ÎµÏ†Î±ÏÎ¼ÏŒÎ¶Ï‰ offset ÏƒÏ„Î± thresholds
specs_adj = []
for spec in model_specs:
    s = spec.copy()
    s["threshold"] = max(0.0, min(1.0, s.get("threshold", 0.5) + thr_offset))
    specs_adj.append(s)

try:
    pred_df, meta = predict_all_models(
        df_engineered=df_eng,
        model_specs=specs_adj,
        default_threshold=0.5,
        keep_proba=True,
        verbose=False
    )
    st.success("OK â€” Î¿Î»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ Ï„Î¿ inference Î³Î¹Î± Ï„Î± ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î± Î¼Î¿Î½Ï„Î­Î»Î±.")
except Exception as e:
    st.error(f"Î£Ï†Î¬Î»Î¼Î± ÏƒÏ„Î¿ inference: {e}")
    st.stop()

# Î“ÏÎ®Î³Î¿ÏÎ· ÏƒÏÎ½Î¿ÏˆÎ·: Ï€ÏŒÏƒÎ± pred=1 Î±Î½Î¬ Î¼Î¿Î½Ï„Î­Î»Î¿ & Ï€Î¹Î¸Î±Î½Î¬ errors
st.write("Î£ÏÎ½Î¿ÏˆÎ· Î¸ÎµÏ„Î¹ÎºÏÎ½ (pred=1) Î±Î½Î¬ Î¼Î¿Î½Ï„Î­Î»Î¿:")
st.dataframe(pred_df.filter(regex="_pred$").sum().to_frame("positives"))

errors = {k: v.get("error") for k, v in meta.items() if v.get("error")}
if errors:
    st.warning(f"ÎœÎ¿Î½Ï„Î­Î»Î± Î¼Îµ ÏƒÏ†Î¬Î»Î¼Î± Ï†ÏŒÏÏ„Ï‰ÏƒÎ·Ï‚: {errors}")

# ==========================
# 4) Alerts & Recommendations
# ==========================
st.subheader("4) Alerts & Recommendations")

alerts_df = compose_alerts_and_recs(pred_df, specs_adj)

# Ï€ÏÏŒÏƒÎ¸ÎµÏƒÎµ timestamp Î±Î½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ ÏƒÏ„Î± engineered Î´ÎµÎ´Î¿Î¼Î­Î½Î±
if "timestamp" in df_eng.columns:
    alerts_df = alerts_df.copy()
    alerts_df["timestamp"] = df_eng["timestamp"].values

cols_show = ["timestamp", "alert_level", "alert_score", "recommendations"] if "timestamp" in alerts_df.columns \
            else ["alert_level", "alert_score", "recommendations"]
st.dataframe(alerts_df[cols_show].head(20))

# ==========================
# 5) KPIs / Î£ÏÎ½Î¿ÏˆÎ·
# ==========================
st.subheader("5) KPIs / Î£ÏÎ½Î¿ÏˆÎ·")

total = len(alerts_df)
active = int((alerts_df["alert_score"] > 0).sum())
rate = active / total if total else 0.0

c1, c2, c3 = st.columns(3)
c1.metric("Î£ÏÎ½Î¿Î»Î¿ Î´ÎµÎ¹Î³Î¼Î¬Ï„Ï‰Î½", f"{total:,}")
c2.metric("Î•Î½ÎµÏÎ³Î¬ alerts", f"{active:,}")
c3.metric("Rate", f"{rate:.2%}")

# ÎšÎ±Ï„Î±Î½Î¿Î¼Î® ÎµÏ€Î¹Ï€Î­Î´Ï‰Î½
level_counts = alerts_df["alert_level"].value_counts().to_dict()
st.write("ÎšÎ±Ï„Î±Î½Î¿Î¼Î® ÎµÏ€Î¹Ï€Î­Î´Ï‰Î½:", level_counts)

# Î£ÏÎ½Î¿ÏˆÎ· Î±Î½Î¬ event/kind + Ï€ÏÏŒÏƒÏ†Î±Ï„Î± ÎµÎ½ÎµÏÎ³Î¬
summary = summarize_for_dashboard(pred_df, alerts_df, specs_adj, top_n=10)
st.write("Î£ÏÎ½Î¿ÏˆÎ· Î±Î½Î¬ event/kind:", summary["events"])
st.write("Î ÏÏŒÏƒÏ†Î±Ï„Î± ÎµÎ½ÎµÏÎ³Î¬:")
st.dataframe(summary["recent"])

# ==========================
# 6) Î“ÏÎ±Ï†Î®Î¼Î±Ï„Î± (Î±Î½Î¬ Î¼Î¿Î½Ï„Î­Î»Î¿)
# ==========================
st.subheader("6) Î“ÏÎ±Ï†Î®Î¼Î±Ï„Î±")

plot_model = st.selectbox("Î”Î¹Î¬Î»ÎµÎ¾Îµ Î¼Î¿Î½Ï„Î­Î»Î¿ Î³Î¹Î± Ï€ÏÎ¿Î²Î¿Î»Î®", options=[m["name"] for m in specs_adj])
prob_col = f"{plot_model}_prob"
pred_col = f"{plot_model}_pred"

# Î“ÏÎ¬Ï†Î·Î¼Î± Ï€Î¹Î¸Î±Î½ÏŒÏ„Î·Ï„Î±Ï‚
if prob_col in pred_df.columns:
    chart_df = pred_df[[prob_col]].copy()
    if "timestamp" in alerts_df.columns:
        chart_df["timestamp"] = alerts_df["timestamp"].values
        st.line_chart(chart_df.set_index("timestamp")[prob_col], height=220)
    else:
        st.line_chart(chart_df[prob_col], height=220)
else:
    st.info("Î¤Î¿ ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿ Î´ÎµÎ½ Ï€Î±ÏÎ­Ï‡ÎµÎ¹ predict_proba.")

# Î Î¯Î½Î±ÎºÎ±Ï‚ ÎµÎ½ÎµÏÎ³ÏÎ½ Î³Î¹Î± Ï„Î¿ ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿
st.subheader("7) Î•Î½ÎµÏÎ³Î¬ alerts Î³Î¹Î± Ï„Î¿ ÎµÏ€Î¹Î»ÎµÎ³Î¼Î­Î½Î¿ Î¼Î¿Î½Ï„Î­Î»Î¿")
mask = pred_df[pred_col] == 1
table_cols = (["timestamp"] if "timestamp" in alerts_df.columns else []) + [pred_col, "alert_level", "recommendations"]
st.dataframe(alerts_df.loc[mask, table_cols].head(200))

# ==========================
# 7) (Î ÏÎ¿Î±Î¹ÏÎµÏ„Î¹ÎºÏŒ) Rolling timeline & debouncing
# ==========================
with st.expander("ğŸ”§ Î ÏÎ¿Î±Î¹ÏÎµÏ„Î¹ÎºÎ¬: Rolling timeline & debouncing"):
    if "timestamp" in alerts_df.columns:
        tl = make_alert_timeline(
            pred_df,
            model_name=plot_model,
            timestamps=alerts_df["timestamp"],
            window_seconds=300  # 5'
        )
        st.write("Î”ÎµÎ¯Î³Î¼Î± timeline (rolling_count):")
        st.dataframe(tl.head())

        thr_count = st.slider("Î•Î»Î¬Ï‡Î¹ÏƒÏ„Î± hits ÏƒÏ„Î¿ Ï€Î±ÏÎ¬Î¸Ï…ÏÎ¿ (rolling_count â‰¥)", 1, 30, 3, 1)
        persist_s = st.slider("Î•Î»Î¬Ï‡Î¹ÏƒÏ„Î· Î´Î¹Î¬ÏÎºÎµÎ¹Î± (Î´ÎµÏ…Ï„.) Î³Î¹Î± debouncing", 1, 60, 5, 1)

        tl2 = apply_alert_rule(
            tl, model_name=plot_model,
            count_threshold=thr_count,
            min_persist_seconds=persist_s
        )
        st.write("Debounced timeline (edge_on = ÏƒÏ„Î¹Î³Î¼Î­Ï‚ Î­Î½Î±ÏÎ¾Î·Ï‚ ÎµÏ€ÎµÎ¹ÏƒÎ¿Î´Î¯Î¿Ï…):")
        st.dataframe(tl2.loc[tl2["edge_on"] == 1].head(30))

        if "ts" in tl2.columns:
            st.line_chart(tl2.set_index("ts")["rolling_count"], height=180)
    else:
        st.info("Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ timestamp â€” Ï„Î¿ timeline Î¼Îµ Î²Î¬ÏƒÎ· Ï‡ÏÏŒÎ½Î¿ ÎµÎ¯Î½Î±Î¹ Î±Ï€ÎµÎ½ÎµÏÎ³Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î¿.")

# ==========================
# 8) Î•Î¾Î±Î³Ï‰Î³Î® Î±Ï€Î¿Ï„ÎµÎ»ÎµÏƒÎ¼Î¬Ï„Ï‰Î½
# ==========================
st.subheader("8) Î•Î¾Î±Î³Ï‰Î³Î®")

export_df = alerts_df.copy()
if "timestamp" in export_df.columns:
    export_df = export_df.sort_values("timestamp")

st.download_button(
    "â¬‡ï¸ ÎšÎ±Ï„Î­Î²Î±ÏƒÎµ Ï„Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± (CSV)",
    data=export_df.to_csv(index=False).encode("utf-8"),
    file_name="alerts_results.csv",
    mime="text/csv"
)


# Auto-refresh ÏƒÏ„Î¿ live
if live_mode:
    st.caption(f"Auto-refresh ÏƒÎµ {refresh_sec}sâ€¦")
    time.sleep(refresh_sec)
    st.experimental_rerun()

